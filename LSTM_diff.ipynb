{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df63f33b-95c6-45ec-97cc-8da41c2d2fcb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081c2e48-bc47-4c9f-94a7-d7d5784acc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "#from model_persistance import ModelPersistance\n",
    "from joblib import dump, load\n",
    "#from evaluate_classification import EvaluateBinaryClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545eb4ff-0a4e-429f-8e14-3fdd4b090926",
   "metadata": {},
   "source": [
    "## Initialise Random variables and Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbaaa1d-c250-4cb2-80ee-e8508409a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71afae85-6cd5-4206-acbb-024ca433a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data_test_clean.csv')\n",
    "df_train = pd.read_csv('data_train_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269bf371-05a9-4c86-84da-5cbdef5aca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train['Tweet_Parsed'].values, df_train['HS'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b199307c-7be7-4567-9370-821ebdf4bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = df_test['Tweet_Parsed'].values, df_test['HS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b4b7e-f986-4132-95ef-21e0e26f1a20",
   "metadata": {},
   "source": [
    "## Transforming data suitable for model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6324d84-27d7-4e1d-8875-546c45ed3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "num_words = 100000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "maxlen = max(map(lambda x: len(x),xtrain))\n",
    "xtrain = pad_sequences(xtrain, maxlen=maxlen)\n",
    "\n",
    "xtest = tokenizer.texts_to_sequences(X_test)\n",
    "xtest = pad_sequences(xtest, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24168a6-e95a-4f5d-8a97-f9b91144832b",
   "metadata": {},
   "source": [
    "## Loading word embedding and mapping data to that word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14af695-0d5c-4dad-b26d-01b3d7f528ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeyedVectors\n\u001b[1;32m----> 2\u001b[0m W2V_BASE \u001b[38;5;241m=\u001b[39m \u001b[43mFastText\u001b[49m\u001b[38;5;241m.\u001b[39mload_fasttext_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Test Feature Expansion/fasttext/cc.id.300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model_ug_cbow \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload(W2V_BASE\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectors.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m embeddings_index \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FastText' is not defined"
     ]
    }
   ],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# W2V_BASE = FastText.load_fasttext_format('../Test Feature Expansion/fasttext/cc.id.300.bin')\n",
    "# model_ug_cbow = KeyedVectors.load(W2V_BASE+'vectors.txt')\n",
    "\n",
    "# embeddings_index = {}\n",
    "# for w in model_ug_cbow.wv.vocab.keys():\n",
    "#     embeddings_index[w] = model_ug_cbow.wv[w]\n",
    "\n",
    "# embedding_matrix = np.zeros((num_words, 200))\n",
    "# for word, i in tokenizer.word_index.items():\n",
    "#     if i >= num_words:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605160f0-a472-4b35-a849-f975d64e6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gilan\\AppData\\Local\\Temp\\ipykernel_30220\\2948567395.py:4: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  model = FastText.load_fasttext_format(model_path)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText\n",
    "model_path = '../Test Feature Expansion/fasttext/cc.id.300.bin'\n",
    "model = FastText.load_fasttext_format(model_path)\n",
    "\n",
    "# Load tokenizer dan tentukan num_words sesuai kebutuhan Anda\n",
    "# tokenizer = ...\n",
    "\n",
    "# Inisialisasi embedding_matrix\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = model.wv[word] if word in model.wv else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10212c78-35ef-4682-afc4-d5318866ecb2",
   "metadata": {},
   "source": [
    "## Creating LSTM model and training it for 10 epoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec46184b-6381-4d2f-9d23-ef784dc1262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "828/828 [==============================] - 330s 396ms/step - loss: 0.5056 - accuracy: 0.7528\n",
      "Epoch 2/3\n",
      "828/828 [==============================] - 261s 315ms/step - loss: 0.3084 - accuracy: 0.8696\n",
      "Epoch 3/3\n",
      "828/828 [==============================] - 256s 309ms/step - loss: 0.1876 - accuracy: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e10afae450>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "\n",
    "def create_lstm_model():\n",
    "    tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "    #tweet_encoder = Embedding(num_words, 200, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
    "    tweet_encoder = Embedding(num_words, 300, input_length=maxlen)(tweet_input)\n",
    "    tweet_encoder = Dropout(0.5)(tweet_encoder)\n",
    "    merged = LSTM(64)(tweet_encoder)\n",
    "    merged = Dropout(0.5)(merged)\n",
    "    merged = Dense(1)(merged)\n",
    "    output = Activation('sigmoid')(merged)\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_model.fit(xtrain, y_train, epochs=3, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753db48d-2f1a-4ce7-99ae-0c8358601292",
   "metadata": {},
   "source": [
    "## Evaluating the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0f7605-79e8-4ca5-a6ae-3b6e46227a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = lstm_model.predict(xtest,verbose=1)\n",
    "# predicted = [int(round(x[0])) for x in p]\n",
    "# actual = y_test\n",
    "\n",
    "# ebc = EvaluateBinaryClassification(gnd_truths = actual, predictions = predicted)\n",
    "# print(ebc.get_full_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83488a9-d18c-4322-87af-b7d2987da228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 2s 10ms/step\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83      2204\n",
      "           1       0.75      0.75      0.75      1476\n",
      "\n",
      "    accuracy                           0.80      3680\n",
      "   macro avg       0.79      0.79      0.79      3680\n",
      "weighted avg       0.80      0.80      0.80      3680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "p = lstm_model.predict(xtest,verbose=1)\n",
    "predicted = [int(round(x[0])) for x in p]\n",
    "actual = y_test\n",
    "\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(actual, predicted, target_names=['0','1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
